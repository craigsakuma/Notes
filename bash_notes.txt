***BASH NOTES

- ls -l #list with details
- ls -ltr #list with details in order of last saved
- ls -a #list with hidden files
- ls -la #list with details and hidden files
- cd ../.. (goes up two directories)
- ./ for current directory
- cp filename ../../dev (saves file in dev folder that's two directories below current directory
- cp -r <target> <destination> copies directory and includes subdirectories and files
- pbcopy < [filename] # command to copy file to clipboard
- mv can be used to rename files
- tab will autocomplete
- clear line at terminal prompt cntrl-u

shortcut keys
- cntrl-u #clears text from terminal line



Find File
- find <path> -name "<filename>"

chmod ### <file>
0 == --- == no access
1 == --x == execute
2 == -w- == write
3 == -wx == write / execute
4 == r-- == read
5 == r-x == read / execute
6 == rw- == read / write
7 == rwx == read / write / execute

pip
- pip freeze #lists all packages installed via pip
- pip install --upgrade [package name] #upgrades existing package
- check version of pip (i.e., might be pip2 install)
- pip install MySQL_python==1.2.2 #install specific version

homebrew
- brew update
- brew upgrade
- brew doctor
- brew list
- files are stored in usr/local/Cellar

.bash_profile
- located in /users/Craig_Sakuma/
- set path.... is used to order paths when searching for files

copy files instead of SCP
- rsync -avP feste.json ec2:  #copy feste.json from local drive to ec2 (requires set up of ec2 in .ssh config file - DONT forget :)
- rsync -avP [target file path] [userid]@[awsnode]:[file path]
- rsync -avP .ssh/awsSec.sec dsack@ec2-23-22-209-93.compute-1.amazonaws.com:.ssh/

check path
- echo $PYTHONPATH

setting up ssh
- config* file in .ssh folder to set ssh shortcuts so you can just type 'ssh ec2' to log in
"""
Host ec2
    HostName ec2-23-22-209-93.compute-1.amazonaws.com
    User dsack 
    IdentityFile <path:file for ssh_key>

Host spark
    HostName ec2-54-242-6-62.compute-1.amazonaws.com
    User root
"""
- chmod so that key is only read for user
- add github ssh key in .ssh folder and follow instructions at github (register key, change url)


Tool for S3 - s3cmd
- command line tool for s3
- allows you to check s3 while in ec2
- example: s3cmd ls  
- example: s3cmd put [filename] s3://test-euclid-dstrauss/  # takes file on ec2 and moves it to s3 in the test-euclid-dstrauss bucket
- s3cmd del -r s3://BUCKET/OBJECT # delete file from bucket recursive
- s3cmd get s3://BUCKET/OBJECT LOCAL_FILE # Get file from bucket


SCREEN COMMAND
- screen # creates remote screen that will continue to run (avoids broken connections)
- cntrl-a, d # leave screen without closing session
- screen -r # re-opens session
- use exit to close screen
- link for more info - https://kb.iu.edu/data/acuy.html


Data Science at the Command Line
by Jeroen Janssens

Links / References
- http://datasciencetoolbox.org/
- http://datascienceatthecommandline.com/
- Thinking with Data by Max Shron

Data Science is OSEMN
- Obtain, Scrub, Explore, Model, iNterpret data

Record commmand line sessions
- https://asciinema.org

Commands
- curl -sL '<url with api request>' # get data from url
- parallel # like multithreading
- 'yada yada yada{0}' :::{2009..2011} # iterate with values
- seq <int> # create sequence
- seq 100 | grep 3 # finds all numbers with a 3 in seq 100
- seq 10 > <directory>/<filename> # output to file
- csvlook  <path/file> | less -S # view csv file
- sqk2csv --db 'sqlite:///data/iris.db' --query 'SELECT * FROM iris WHERE sepal_length > 7.5' # query to csv
- sqk2csv --db 'sqlite:///data/iris.db' --query 'SELECT * FROM iris WHERE sepal_length > 7.5' | csvlook # view output in pretty format
- curl -s http://www.gutenberg.org/cache/epub/76/pg76.txt | tr '[:upper:]' '[:lower:]' | grep -oE '\w+' | sort | uniq -c | sort -nr | head -n 10   # get top 10 words 

- shebang "#!/usr/bin/env bash" # makes file executable in designated language
- remove curl statement and the file will take input 

